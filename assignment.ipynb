{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Raw texts into training and development data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dev = pd.read_csv('dev.tsv', sep='\\t')\n",
    "data_test = pd.read_csv('test.tsv', sep='\\t')\n",
    "data_train = pd.read_csv('train.tsv', sep='\\t')\n",
    "\n",
    "data_dev_phrases = list(data_dev['Phrase'])\n",
    "data_test_phrases = list(data_test['Phrase'])\n",
    "data_train_phrases = list(data_train['Phrase'])\n",
    "\n",
    "data_dev_sentiments_5 = list(data_dev['Sentiment'])\n",
    "data_train_sentiments_5 = list(data_train['Sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map to 3-value Sentiment Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_3_value(sentiments):\n",
    "    value_scale = {\n",
    "        0: 0,\n",
    "        1: 0,\n",
    "        2: 1,\n",
    "        3: 2,\n",
    "        4: 2,\n",
    "    }\n",
    "    \n",
    "    return [value_scale[sentiment] for sentiment in sentiments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 1, 3, 1, 4, 1, 3, 1, 1, 1, 1, 4, 3, 3, 3, 3, 2, 1, 2]\n",
      "[0, 2, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "data_dev_sentiments_3 = map_to_3_value(data_dev_sentiments_5)\n",
    "data_train_sentiments_3 = map_to_3_value(data_train_sentiments_5)\n",
    "\n",
    "print(data_train_sentiments_5[:20])\n",
    "print(data_train_sentiments_3[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Pre-Processing\n",
    "\n",
    "1. Tokenisation\n",
    "2. Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_stop_words = {\n",
    "    'a', 'ad', 'after', 'again', 'all', 'also', 'am', 'an', 'and', 'any',\n",
    "    'are', 'as', 'at', 'be', 'because', 'been', 'being', 'between', 'both',\n",
    "    'but', 'by', 'can', 'could', 'does', 'each', 'ed', 'eg', 'either', 'etc',\n",
    "    'even', 'ever', 'every', 'for', 'from', 'had', 'has', 'have', 'he', 'her',\n",
    "    'hers', 'herself', 'him', 'himself', 'his', 'i', 'ie', 'if', 'in', 'inc',\n",
    "    'into', 'is', 'it', 'its', 'itself', 'li', 'll', 'ltd', 'may', 'maybe',\n",
    "    'me', 'might', 'mine', 'minute', 'minutes', 'must', 'my', 'myself',\n",
    "    'neither', 'nor', 'now', 'of', 'on', 'only', 'or', 'other', 'our', 'ours',\n",
    "    'ourselves', 'own', 'same', 'seem', 'seemed', 'shall', 'she', 'some',\n",
    "    'somehow', 'something', 'sometimes', 'somewhat', 'somewhere', 'spoiler',\n",
    "    'spoilers', 'such', 'suppose', 'that', 'the', 'their', 'theirs', 'them',\n",
    "    'themselves', 'there', 'these', 'they', 'this', 'those', 'thus', 'to',\n",
    "    'today', 'tomorrow', 'us', 've', 'vs', 'was', 'we', 'were', 'what',\n",
    "    'whatever', 'when', 'where', 'which', 'who', 'whom', 'whose', 'will',\n",
    "    'with', 'yesterday', 'you', 'your', 'yours', 'yourself', 'yourselves'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram extraction from a phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unigrams(phrase):\n",
    "    return [\n",
    "        word.lower() for word in re.findall(r'\\b[A-Za-z]{2,}\\b', phrase)\n",
    "        if word.lower() not in default_stop_words\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary of unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(phrases):\n",
    "    return {\n",
    "        word.lower() for phrase in phrases \n",
    "        for word in re.findall(r'\\b[A-Za-z]{2,}\\b', phrase)\n",
    "        if word.lower() not in default_stop_words\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accompanies', 'existential', 'wretchedly', 'partners', 'providing', 'friggin', 'times', 'susan', 'dual', 'pressed', 'available', 'ramblings', 'plainness', 'text', 'tightrope', 'load', 'agreeably', 'underdeveloped', 'dismissive', 'walls', 'screens', 'infatuation', 'plucky', 'bruce', 'giddy', 'red', 'inquisitive', 'thurman', 'flops', 'absorption', 'joan', 'detention', 'simmer', 'profane', 'schumacher', 'topple', 'beers', 'plunging', 'whirlwind', 'bardem', 'entranced', 'accentuating', 'rejected', 'enthusiasts', 'directors', 'turkey', 'snappy', 'cars', 'thoughtlessly', 'butthead']\n"
     ]
    }
   ],
   "source": [
    "vocab = { unigram for phrase in data_train_phrases \n",
    "         for unigram in extract_unigrams(phrase) }\n",
    "\n",
    "print(list(vocab)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create vocabulary id -> word and word -> id dictionaries for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_id_to_word = dict(enumerate(vocab))\n",
    "\n",
    "word_to_vocab_id = {v: k for k, v in vocab_id_to_word.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract unigrams for each phrase in development, train, and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dev_unigrams = [extract_unigrams(phrase) for phrase in data_dev_phrases]\n",
    "\n",
    "data_train_unigrams = [extract_unigrams(phrase) for phrase in data_train_phrases]\n",
    "\n",
    "data_test_unigrams = [extract_unigrams(phrase) for phrase in data_test_phrases]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the prior probability of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prior_probability(sentiments):    \n",
    "    counts = Counter(sentiments)\n",
    "    \n",
    "    total = len(sentiments)\n",
    "    \n",
    "    return { k: v / total for (k, v) in sorted(counts.items()) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dev_5_prob = calculate_prior_probability(data_dev_sentiments_5)\n",
    "data_train_5_prob = calculate_prior_probability(data_train_sentiments_5)\n",
    "\n",
    "data_dev_3_prob = calculate_prior_probability(data_dev_sentiments_3)\n",
    "data_train_3_prob = calculate_prior_probability(data_train_sentiments_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the probability of each word in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_probability(total_vocab, data_unigrams, sentiments):\n",
    "    array = np.zeros((total_vocab, len(set(sentiments))))\n",
    "    \n",
    "    for i, unigrams in enumerate(data_unigrams):\n",
    "        for word, count in Counter(unigrams).items():\n",
    "            array[word_to_vocab_id[word]][sentiments[i]] += count\n",
    "\n",
    "    # With Laplace smoothing\n",
    "    return (array[:] + 1) / (array.sum(axis=0) + total_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-value Sentiment scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.31921703e-05, 7.13546684e-05, 4.28862442e-05],\n",
       "       [4.63843406e-05, 3.56773342e-05, 1.07215611e-04],\n",
       "       [4.63843406e-05, 3.56773342e-05, 2.14431221e-05],\n",
       "       ...,\n",
       "       [4.63843406e-05, 3.56773342e-05, 2.14431221e-05],\n",
       "       [2.31921703e-05, 7.13546684e-05, 2.14431221e-05],\n",
       "       [9.27686813e-05, 3.56773342e-05, 4.28862442e-05]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_prob_3 = calculate_word_probability(len(vocab), data_train_unigrams, data_train_sentiments_3)\n",
    "\n",
    "word_prob_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-value Sentiment scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.27953952e-05, 2.96068214e-05, 7.13546684e-05, 5.67391983e-05,\n",
       "        3.93530361e-05],\n",
       "       [4.27953952e-05, 5.92136428e-05, 3.56773342e-05, 1.41847996e-04,\n",
       "        3.93530361e-05],\n",
       "       [4.27953952e-05, 5.92136428e-05, 3.56773342e-05, 2.83695991e-05,\n",
       "        3.93530361e-05],\n",
       "       ...,\n",
       "       [4.27953952e-05, 5.92136428e-05, 3.56773342e-05, 2.83695991e-05,\n",
       "        3.93530361e-05],\n",
       "       [4.27953952e-05, 2.96068214e-05, 7.13546684e-05, 2.83695991e-05,\n",
       "        3.93530361e-05],\n",
       "       [8.55907904e-05, 8.88204642e-05, 3.56773342e-05, 5.67391983e-05,\n",
       "        3.93530361e-05]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_prob_5 = calculate_word_probability(len(vocab), data_train_unigrams, data_train_sentiments_5)\n",
    "\n",
    "word_prob_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict development sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiments(prior_prob, word_prob, data_unigrams, true_sentiments):\n",
    "    total_sentiments = len(set(true_sentiments))\n",
    "    \n",
    "    confusion_matrix = np.zeros((total_sentiments, total_sentiments))\n",
    "    \n",
    "    # X-axis is predicted sentiments, Y-axis is true sentiments\n",
    "    \n",
    "    \n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-value Sentiment scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sentiments_3 = predict_sentiments(data_train_3_prob, word_prob_3, data_dev_unigrams, data_dev_sentiments_3)\n",
    "\n",
    "pred_sentiments_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-value Sentiment scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sentiments_5 = predict_sentiments(data_train_5_prob, word_prob_5, data_dev_unigrams, data_dev_sentiments_5)\n",
    "\n",
    "pred_sentiments_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict test sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-value Sentiment scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-value Sentiment scale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
